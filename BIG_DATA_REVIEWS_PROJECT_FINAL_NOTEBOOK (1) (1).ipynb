{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9dccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32214e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "spark_context = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "71fb13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"D:/Reviews.csv\",header=True,inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "1eaf246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: string (nullable = true)\n",
      " |-- HelpfulnessDenominator: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "7dc06e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('Text','Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "4e7f7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4412b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00ab7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "483b1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Text|Score|\n",
      "+--------------------+-----+\n",
      "|I have bought sev...|    5|\n",
      "|\"Product arrived ...|    1|\n",
      "|\"This is a confec...|    4|\n",
      "|If you are lookin...|    2|\n",
      "|Great taffy at a ...|    5|\n",
      "|I got a wild hair...|    4|\n",
      "|This saltwater ta...|    5|\n",
      "|This taffy is so ...|    5|\n",
      "|Right now I'm mos...|    5|\n",
      "|This is a very he...|    5|\n",
      "|I don't know if i...|    5|\n",
      "|One of my boys ne...|    5|\n",
      "|My cats have been...|    1|\n",
      "|good flavor! thes...|    4|\n",
      "|The Strawberry Tw...|    5|\n",
      "|My daughter loves...|    5|\n",
      "|I love eating the...|    2|\n",
      "|I am very satisfi...|    5|\n",
      "|Twizzlers, Strawb...|    5|\n",
      "|Candy was deliver...|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fbf7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "df_3 = df.withColumn(\"new_column\",\n",
    "           when((col(\"Score\") == \"3\") | (col(\"Score\") == \"2\") | (col(\"Score\") == \"1\"), 0)\n",
    "      .when((col(\"Score\") == \"4\") | (col(\"Score\") == \"5\"), 1)\n",
    "      .otherwise(\"new_df.Score\"))\n",
    "df_4 = df_3.drop(df_3.Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d501df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Text|new_column|\n",
      "+--------------------+----------+\n",
      "|I have bought sev...|         1|\n",
      "|\"Product arrived ...|         0|\n",
      "|\"This is a confec...|         1|\n",
      "|If you are lookin...|         0|\n",
      "|Great taffy at a ...|         1|\n",
      "|I got a wild hair...|         1|\n",
      "|This saltwater ta...|         1|\n",
      "|This taffy is so ...|         1|\n",
      "|Right now I'm mos...|         1|\n",
      "|This is a very he...|         1|\n",
      "|I don't know if i...|         1|\n",
      "|One of my boys ne...|         1|\n",
      "|My cats have been...|         0|\n",
      "|good flavor! thes...|         1|\n",
      "|The Strawberry Tw...|         1|\n",
      "|My daughter loves...|         1|\n",
      "|I love eating the...|         0|\n",
      "|I am very satisfi...|         1|\n",
      "|Twizzlers, Strawb...|         1|\n",
      "|Candy was deliver...|         1|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "577c6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "new_dataset = df_4.withColumn(\"new_column\",col(\"new_column\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4ba8ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 397778\n",
      "Test Dataset Count: 170666\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = new_dataset.randomSplit([0.7, 0.3])\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "451e2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "21aee3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = Tokenizer(outputCol = \"words\").setInputCol(\"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "82e45599",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData = tokenizer.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1937f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "tf = hashingTF.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7aefb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(tf) \n",
    "tfidf = idfModel.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2c3c22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3e878",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "072b15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol='new_column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "bbbeaad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lrModel = lr.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7304a97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_01adfd43b472, numClasses=2, numFeatures=262144"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c817a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = lrModel.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b960fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                Text|new_column|               words|         rawFeatures|            features|       rawPrediction|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            almost.\"|         1|        [, almost.\"]|(262144,[242002,2...|(262144,[242002,2...|[-1.0185823196254...|       1.0|\n",
      "| but made from 10...|         1|[, but, made, fro...|(262144,[33917,70...|(262144,[33917,70...|[-1.0060223991956...|       1.0|\n",
      "| easy and delicio...|         1|[, easy, and, del...|(262144,[96984,20...|(262144,[96984,20...|[-1.1649944549642...|       1.0|\n",
      "|    easy to prepare\"|         1|[, easy, to, prep...|(262144,[27576,30...|(262144,[27576,30...|[-1.0648722594166...|       1.0|\n",
      "|      says cat.  :)\"|         1|[, says, cat., , ...|(262144,[89832,16...|(262144,[89832,16...|[-1.0238779446372...|       1.0|\n",
      "| this is the kind...|         1|[, this, is, the,...|(262144,[70065,79...|(262144,[70065,79...|[-1.0321916325317...|       1.0|\n",
      "|\"\"\"And God gave m...|         1|[\"\"\"and, god, gav...|(262144,[10860,17...|(262144,[10860,17...|[-1.2009638846858...|       1.0|\n",
      "|\"\"\"Best syrup I'v...|         0|[\"\"\"best, syrup, ...|(262144,[37245,10...|(262144,[37245,10...|[4.46484728313235...|       0.0|\n",
      "|\"\"\"Bottom Line Up...|         1|[\"\"\"bottom, line,...|(262144,[2030,777...|(262144,[2030,777...|[-1.0592059532011...|       1.0|\n",
      "|\"\"\"Chocolate with...|         0|[\"\"\"chocolate, wi...|(262144,[8917,105...|(262144,[8917,105...|[7.80077911051693...|       0.0|\n",
      "|\"\"\"Draught of the...|         1|[\"\"\"draught, of, ...|(262144,[30950,35...|(262144,[30950,35...|[-1.0199739870023...|       1.0|\n",
      "|\"\"\"February made ...|         0|[\"\"\"february, mad...|(262144,[26131,50...|(262144,[26131,50...|[4.81792642990415...|       0.0|\n",
      "|\"\"\"I love them\"\"<...|         1|[\"\"\"i, love, them...|(262144,[25629,38...|(262144,[25629,38...|[-1.4190715546488...|       1.0|\n",
      "|\"\"\"Ingredients:<b...|         0|[\"\"\"ingredients:<...|(262144,[45996,67...|(262144,[45996,67...|[4.42667446053103...|       0.0|\n",
      "|\"\"\"Lipton To Go S...|         1|[\"\"\"lipton, to, g...|(262144,[9358,275...|(262144,[9358,275...|[-1.0552382578272...|       1.0|\n",
      "|\"\"\"NOW\"\" brand st...|         1|[\"\"\"now\"\", brand,...|(262144,[17772,19...|(262144,[17772,19...|[-2.9231993412351...|       1.0|\n",
      "|\"\"\"Nantucket Blen...|         1|[\"\"\"nantucket, bl...|(262144,[10044,12...|(262144,[10044,12...|[-3.9832230602207...|       1.0|\n",
      "|\"\"\"Newman's Own\"\"...|         1|[\"\"\"newman's, own...|(262144,[58267,74...|(262144,[58267,74...|[-1.1594499477744...|       1.0|\n",
      "|\"\"\"Rub with Love ...|         1|[\"\"\"rub, with, lo...|(262144,[1109,356...|(262144,[1109,356...|[-4.4750335515842...|       1.0|\n",
      "|\"\"\"Sam\"\" loves th...|         1|[\"\"\"sam\"\", loves,...|(262144,[1797,387...|(262144,[1797,387...|[-1.5492332991673...|       1.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a02c4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_train.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d180a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(testData)\n",
    "test_tf = hashingTF.transform(test_tokens)\n",
    "test_tfidf = idfModel.transform(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "1f994eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = test_tfidf.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6b50bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lrModel.transform(test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "200c5b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|new_column|prediction|\n",
      "+----------+----------+\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       0.0|\n",
      "|         1|       1.0|\n",
      "|         0|       1.0|\n",
      "|         0|       1.0|\n",
      "|         0|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         0|       0.0|\n",
      "|         1|       1.0|\n",
      "|         0|       0.0|\n",
      "|         0|       0.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "|         1|       1.0|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select('new_column','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "624dc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"new_column\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "AUC = evaluator.evaluate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "5f83ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = res[(res.new_column==1) & (res.prediction==1)].count()\n",
    "tn = res[(res.new_column==0) & (res.prediction==0)].count()\n",
    "fp = res[(res.new_column==0) & (res.prediction==1)].count()\n",
    "fn = res[(res.new_column==1) & (res.prediction==0)].count()\n",
    "accuracy = float((tp+tn)/(res.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d7b99bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 313187\n",
      "True Negatives: 74099\n",
      "False Positives: 26137\n",
      "False Negatives: 40364\n",
      "AUC: 0.8125389882406717\n",
      "Accuracy: 0.8534532721298759\n",
      "Recall: 0.8858325955802699\n",
      "Precision:  0.9229733234312928\n",
      "F1 score: 0.9040216489265741\n"
     ]
    }
   ],
   "source": [
    "if(tp + fn == 0.0):\n",
    "    r = 0.0\n",
    "    p = float(tp) / (tp + fp)\n",
    "elif(tp + fp == 0.0):\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = 0.0\n",
    "else:\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = float(tp) / (tp + fp)\n",
    "    \n",
    "if(p + r == 0):\n",
    "    f1 = 0\n",
    "else:\n",
    "    f1 = 2 * ((p * r)/(p + r))\n",
    "\n",
    "    \n",
    "\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"AUC:\",AUC)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", r)\n",
    "print(\"Precision: \", p)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cc04e",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b37ff79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSVC = LinearSVC(featuresCol=\"features\", labelCol='new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0352d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSVC_MODEL = LSVC.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a1fb263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = LSVC_MODEL.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "bc130681",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(testData)\n",
    "test_tf = hashingTF.transform(test_tokens)\n",
    "test_tfidf = idfModel.transform(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "cef1ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = test_tfidf.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b0c14232",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = LSVC_MODEL.transform(test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "bb536c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"new_column\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "AUC = evaluator.evaluate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "303ebf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = res[(res.new_column==1) & (res.prediction==1)].count()\n",
    "tn = res[(res.new_column==0) & (res.prediction==0)].count()\n",
    "fp = res[(res.new_column==0) & (res.prediction==1)].count()\n",
    "fn = res[(res.new_column==1) & (res.prediction==0)].count()\n",
    "accuracy = float((tp+tn)/(res.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8b7d26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 200463\n",
      "True Negatives: 48477\n",
      "False Positives: 14293\n",
      "False Negatives: 20366\n",
      "AUC: 0.8400352383611893\n",
      "Accuracy: 0.8777887087048967\n",
      "Recall: 0.9077747940714308\n",
      "Precision:  0.9334453984987614\n",
      "F1 score: 0.9204311443231517\n"
     ]
    }
   ],
   "source": [
    "if(tp + fn == 0.0):\n",
    "    r = 0.0\n",
    "    p = float(tp) / (tp + fp)\n",
    "elif(tp + fp == 0.0):\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = 0.0\n",
    "else:\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = float(tp) / (tp + fp)\n",
    "    \n",
    "if(p + r == 0):\n",
    "    f1 = 0\n",
    "else:\n",
    "    f1 = 2 * ((p * r)/(p + r))\n",
    "\n",
    "    \n",
    "\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"AUC:\",AUC)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", r)\n",
    "print(\"Precision: \", p)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a7317",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0c7e8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(featuresCol=\"features\", labelCol='new_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "142f92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_MODEL = NB.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "663d8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = NB_MODEL.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "464b57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(testData)\n",
    "test_tf = hashingTF.transform(test_tokens)\n",
    "test_tfidf = idfModel.transform(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "29086cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = test_tfidf.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "4835beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = NB_MODEL.transform(test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d3c8e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"new_column\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "AUC = evaluator.evaluate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3742a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = res[(res.new_column==1) & (res.prediction==1)].count()\n",
    "tn = res[(res.new_column==0) & (res.prediction==0)].count()\n",
    "fp = res[(res.new_column==0) & (res.prediction==1)].count()\n",
    "fn = res[(res.new_column==1) & (res.prediction==0)].count()\n",
    "accuracy = float((tp+tn)/(res.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "63962a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 116797\n",
      "True Negatives: 28505\n",
      "False Positives: 9121\n",
      "False Negatives: 15764\n",
      "AUC: 0.8193345004157759\n",
      "Accuracy: 0.8537784907190326\n",
      "Recall: 0.8810811626345607\n",
      "Precision:  0.9275639702028304\n",
      "F1 score: 0.9037252542759759\n"
     ]
    }
   ],
   "source": [
    "if(tp + fn == 0.0):\n",
    "    r = 0.0\n",
    "    p = float(tp) / (tp + fp)\n",
    "elif(tp + fp == 0.0):\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = 0.0\n",
    "else:\n",
    "    r = float(tp) / (tp + fn)\n",
    "    p = float(tp) / (tp + fp)\n",
    "    \n",
    "if(p + r == 0):\n",
    "    f1 = 0\n",
    "else:\n",
    "    f1 = 2 * ((p * r)/(p + r))\n",
    "\n",
    "    \n",
    "\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"AUC:\",AUC)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", r)\n",
    "print(\"Precision: \", p)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013bedf",
   "metadata": {},
   "source": [
    "# ML PIPELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "eng_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, CountVectorizer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer  = Tokenizer(inputCol = 'Text',outputCol = \"tokens\")\n",
    "stopwordsremover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "vectorizer = CountVectorizer(inputCol = 'tokens', outputCol = 'raw_features')\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol = 'raw_features', outputCol = 'vectorizedfeatures')\n",
    "lr = LogisticRegression(featuresCol=\"vectorizedfeatures\", labelCol='Score')\n",
    "lr_pipeline = Pipeline(stages = [tokenizer,stopwordsremover,vectorizer,hashingTF,idf,lr ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bdc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
